{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC-jFqvE353y"
      },
      "source": [
        "# Tweet Scrapping and extraction\n",
        "This is the first note book where I did the extraction of the tweets used  for the project, in the second notebook, I did a detailed exploratory data analysis and sentiment analysis, yoou can check it here.\n",
        "I also wrote a story-like report report which you can check out on medium."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsAMB-bU7yxX",
        "outputId": "086f5de5-91c0-4ac3-bf38-fcbb2fddf319"
      },
      "source": [
        "\n",
        "!pip install twitterscraper"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twitterscraper\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/d9/71aad697f3853be116aa26b83a80ce1b374deb2c65473e42e3a020f9b880/twitterscraper-1.6.1.tar.gz\n",
            "Collecting coala-utils~=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/54/00/74ec750cfc4e830f9d1cfdd4d559f3d2d4ba1b834b78d5266446db3fd1d6/coala_utils-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from twitterscraper) (0.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from twitterscraper) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from twitterscraper) (2.23.0)\n",
            "Collecting billiard\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/5e/7591866ff45b370354bd20291cb6f87ddb2eef1f1c88c890a38412037e11/billiard-3.6.3.0-py3-none-any.whl (89kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->twitterscraper) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->twitterscraper) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->twitterscraper) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->twitterscraper) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->twitterscraper) (3.0.4)\n",
            "Building wheels for collected packages: twitterscraper\n",
            "  Building wheel for twitterscraper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twitterscraper: filename=twitterscraper-1.6.1-cp37-none-any.whl size=11347 sha256=82023622e5e85b266bca072d49bb992d9d61d82e54afbb217851df0365bb10ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/50/a0/808fd6df0ac14c168b73874056a1ca52e4d88c0c1ecedd4e48\n",
            "Successfully built twitterscraper\n",
            "Installing collected packages: coala-utils, billiard, twitterscraper\n",
            "Successfully installed billiard-3.6.3.0 coala-utils-0.5.1 twitterscraper-1.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfRQ8Rr4w7pp",
        "outputId": "42774f0d-c61b-4e6e-c9a5-c7f763778e18"
      },
      "source": [
        "#To read the uploaded file that contains the key we need configparser\n",
        "!pip install ConfigParser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ConfigParser\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Installing collected packages: ConfigParser\n",
            "Successfully installed ConfigParser-5.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2o01jAhFPS"
      },
      "source": [
        "So I already uploaded my api and access keys and scretes to my drive in a .txt file named tweeter.api.properties, it could be anyname though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNqGAAuA3rp4",
        "outputId": "d40003dd-7ad4-480d-c252-db2a99a00c02"
      },
      "source": [
        "import configparser\n",
        "config = configparser.RawConfigParser()\n",
        "config.read(filenames = '/content/drive/MyDrive/Colab Notebooks/Twitterapi.properties.txt')\n",
        "#getting all hthe sections in the file\n",
        "print(config.sections())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['twitter']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrvLk_CN6UaO"
      },
      "source": [
        "#import the neccessary libraries\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cy-W4B36-UW"
      },
      "source": [
        "#accessing API \n",
        "accesstoken = config.get('twitter', 'accesstoken') # config.get takes the aurgement 'section' and  'the line title'\n",
        "accesstokensecrete = config.get('twitter', 'accesstokensecret')\n",
        "apikey = config.get('twitter', 'apikey')\n",
        "apikeysecret = config.get('twitter', 'apikeysecret')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nt0DDGVh9ve"
      },
      "source": [
        "### For the first Exctration method, I will be calling the twitter authenticator directly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9mVg-80qTAr"
      },
      "source": [
        "\n",
        "auth = OAuthHandler(apikey, apikeysecret)\n",
        "auth.set_access_token(accesstoken, accesstokensecrete)\n",
        "api = tweepy.API(auth)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEBznFby8YKo"
      },
      "source": [
        "#creating a function that does all the scrapping and coverts to a csv file\n",
        "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
        "    \n",
        "    # Define a for-loop to generate tweets at regular intervals\n",
        "    # We cannot make large API call in one go. Hence, let's try T times\n",
        "    \n",
        "    # Define a pandas dataframe to store the data:\n",
        "    global  db_tweets\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags'])\n",
        "    program_start = time.time()\n",
        "    for i in range(0, numRuns):\n",
        "      # We will time how long it takes to scrape tweets for each run:\n",
        "      start_run = time.time()\n",
        "\n",
        "      # Collect tweets using the Cursor object\n",
        "      # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
        "      # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
        "      \n",
        "      tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
        "      \n",
        "      # Store these tweets into a python list\n",
        "      tweet_list = [tweet for tweet in tweets]\n",
        "      \n",
        "      \n",
        "      # Obtain the following info (methods to call them out):\n",
        "      # user.screen_name - twitter handle\n",
        "      # user.description - description of account\n",
        "      # user.location - where is he tweeting from\n",
        "      # user.friends_count - no. of other users that user is following (following)\n",
        "      # user.followers_count - no. of other users who are following this user (followers)\n",
        "      # user.statuses_count - total tweets by user\n",
        "      # user.created_at - when the user account was created\n",
        "      # created_at - when the tweet was created\n",
        "      # retweet_count - no. of retweets\n",
        "      # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
        "      # retweeted_status.full_text - full text of the tweet\n",
        "      # tweet.entities['hashtags'] - hashtags in the tweet\n",
        "\n",
        "      # Begin scraping the tweets individually:\n",
        "      noTweets = 0\n",
        "      for tweet in tweet_list:\n",
        "        # Pull the values\n",
        "        username = tweet.user.screen_name\n",
        "        acctdesc = tweet.user.description\n",
        "        location = tweet.user.location\n",
        "        following = tweet.user.friends_count\n",
        "        followers = tweet.user.followers_count\n",
        "        totaltweets = tweet.user.statuses_count\n",
        "        usercreatedts = tweet.user.created_at\n",
        "        tweetcreatedts = tweet.created_at\n",
        "        retweetcount = tweet.retweet_count\n",
        "        hashtags = tweet.entities['hashtags']\n",
        "        try:\n",
        "            text = tweet.retweeted_status.full_text\n",
        "        except AttributeError:  # Not a Retweet\n",
        "            text = tweet.full_text\n",
        "        # Add the 11 variables to the empty list - ith_tweet:\n",
        "        ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "        # Append to dataframe - db_tweets\n",
        "        db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "        # increase counter - noTweets  \n",
        "        noTweets += 1\n",
        "        \n",
        "      # Run ended:\n",
        "      end_run = time.time()\n",
        "      duration_run = round((end_run-start_run)/60, 2)\n",
        "      \n",
        "      print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
        "      print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
        "          \n",
        "      if i < numRuns:\n",
        "        time.sleep(920) #15 minute sleep time\n",
        "    # Once all runs have completed, save them to a single csv file:\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Obtain timestamp in a readable format\n",
        "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
        "    # Define working path and filename\n",
        "    path = os.getcwd()\n",
        "    filename = '/content/drive/MyDrive/Colab Notebooks/' + to_csv_timestamp + search_words + '_ogbeni_tweets.csv'\n",
        "    # Store dataframe in csv with creation date timestamp\n",
        "    db_tweets.to_csv(filename, index = False)\n",
        "\n",
        "    program_end = time.time()\n",
        "    print('Scraping has completed!')\n",
        "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txXqrKP70V6n"
      },
      "source": [
        "# Initialise these variables:\n",
        "search_words = \"@ogbenidipo OR Dr Dipo OR Ogbeni Dipo\"\n",
        "date_since = \"2021-03-23\"\n",
        "numTweets = 3200\n",
        "numRuns = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjl8V430-LLg",
        "outputId": "5f6f2cf6-a3b8-4949-9447-31e683c69b36"
      },
      "source": [
        "\n",
        "# Call the function scraptweets\n",
        "scraptweets(search_words, date_since, numTweets, numRuns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of tweets scraped for run 1 is 786\n",
            "time take for 1 run to complete is 0.35 mins\n",
            "no. of tweets scraped for run 2 is 786\n",
            "time take for 2 run to complete is 0.35 mins\n",
            "no. of tweets scraped for run 3 is 786\n",
            "time take for 3 run to complete is 0.37 mins\n",
            "Scraping has completed!\n",
            "Total time taken to scrap is 47.083333333333336 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_g0vRoWiz_e"
      },
      "source": [
        "###Second Extration method by calling the authenticator via tweepy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MoZ_wgAjsYE"
      },
      "source": [
        "auth = tweepy.OAuthHandler(apikey,apikeysecret) \n",
        "auth.set_access_token(accesstoken,accesstokensecrete) \n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "search_word = '@ogbenidipo OR Dr Dipo OR Ogbeni Dipo' \n",
        "date_since = '2021-03-31' # tweepy starts extraction from a day after the date-since\n",
        "tweets = tweepy.Cursor(api.search,q = search_word, lang ='en',since = date_since).items(2000)\n",
        "tweet_details = [[tweet.user.screen_name, tweet.user.location, tweet.favorite_count, tweet.user.followers_count, tweet.user.statuses_count,\n",
        "                         tweet.user.created_at, tweet.created_at, tweet.retweet_count, tweet.text]for tweet in tweets]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAhkPpldejmh",
        "outputId": "15167d15-b647-422d-8388-8142012ccba8"
      },
      "source": [
        "print (len(tweet_details))\n",
        "complex_tweets = pd.DataFrame(tweet_details, columns = ['username', 'location', 'likes',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5QHhy-sgIep"
      },
      "source": [
        "complex_tweets.to_csv('/content/drive/MyDrive/Colab Notebooks/'+ date_since + search_word, index= False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR0p491ggnHo"
      },
      "source": [
        "complex_tweets= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/' + date_since + search_word, parse_dates=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "My6kaspPgFbb",
        "outputId": "ec448a84-5f26-44ab-c0a0-9e991df542a0"
      },
      "source": [
        "## a sample of what the dataframw looks like\n",
        "complex_tweets.tail()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>location</th>\n",
              "      <th>likes</th>\n",
              "      <th>followers</th>\n",
              "      <th>totaltweets</th>\n",
              "      <th>usercreatedts</th>\n",
              "      <th>tweetcreatedts</th>\n",
              "      <th>retweetcount</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>637</th>\n",
              "      <td>fuski_</td>\n",
              "      <td>Portharcourt</td>\n",
              "      <td>0</td>\n",
              "      <td>156</td>\n",
              "      <td>2882</td>\n",
              "      <td>2013-01-15 10:11:32</td>\n",
              "      <td>2021-03-31 08:32:09</td>\n",
              "      <td>0</td>\n",
              "      <td>@OgbeniDipo Oga Dipo still dey beg oo...e say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>fuski_</td>\n",
              "      <td>Portharcourt</td>\n",
              "      <td>76</td>\n",
              "      <td>156</td>\n",
              "      <td>2882</td>\n",
              "      <td>2013-01-15 10:11:32</td>\n",
              "      <td>2021-03-31 08:28:13</td>\n",
              "      <td>0</td>\n",
              "      <td>@OgbeniDipo Oga Dipo say all those young guys ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>PrinceFromDilli</td>\n",
              "      <td>üåê</td>\n",
              "      <td>0</td>\n",
              "      <td>204</td>\n",
              "      <td>7747</td>\n",
              "      <td>2013-03-26 13:10:22</td>\n",
              "      <td>2021-03-31 05:39:48</td>\n",
              "      <td>1</td>\n",
              "      <td>@guardian I don't know what this guy Dr dipo i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>640</th>\n",
              "      <td>ibjnr21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "      <td>3981</td>\n",
              "      <td>2014-07-05 20:47:34</td>\n",
              "      <td>2021-03-31 04:49:42</td>\n",
              "      <td>0</td>\n",
              "      <td>@OgbeniDipo Gbam Mr Dipo, you have said it all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>641</th>\n",
              "      <td>oz_enenia</td>\n",
              "      <td>Enugu</td>\n",
              "      <td>0</td>\n",
              "      <td>185</td>\n",
              "      <td>1241</td>\n",
              "      <td>2012-06-17 12:09:54</td>\n",
              "      <td>2021-03-31 00:07:39</td>\n",
              "      <td>0</td>\n",
              "      <td>@brainbizinfo @OgbeniDipo @akinalabi Will do. ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            username  ...                                               text\n",
              "637           fuski_  ...  @OgbeniDipo Oga Dipo still dey beg oo...e say ...\n",
              "638           fuski_  ...  @OgbeniDipo Oga Dipo say all those young guys ...\n",
              "639  PrinceFromDilli  ...  @guardian I don't know what this guy Dr dipo i...\n",
              "640          ibjnr21  ...     @OgbeniDipo Gbam Mr Dipo, you have said it all\n",
              "641        oz_enenia  ...  @brainbizinfo @OgbeniDipo @akinalabi Will do. ...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlO0Tuguknss"
      },
      "source": [
        "#Continue in the Second notebook here"
      ]
    }
  ]
}